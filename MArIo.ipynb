{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_analysis import ts2xy\n",
    "from stable_baselines3.common.results_analysis import load_results\n",
    "import numpy as np\n",
    "from stable_baselines3 import A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "for step in range(1000):\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4, channels_order='chw')\n",
    "#add a neural network of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state, reward, done, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the first 5 frames\n",
    "plt.figure(figsize=(16, 8))\n",
    "for index, frame in enumerate(state[:6]):\n",
    "    plt.subplot(2, 3, index + 1)\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(trainingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        if self.verbose > 0:\n",
    "            print(\"Training callback is initialized\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model to {}\".format(self.log_dir))\n",
    "                    self.model.save(self.log_dir + 'best_model')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = ''\n",
    "logs_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = trainingCallback(check_freq=1000, save = checkpoint_dir, log_dir = logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = PPO('CnnPolicy', env, verbose=1, tensorboard_log=logs_dir, learning_rate= 0.03 n_steps= 256, batch_size= 256, n_epochs= 10, gamma= 0.99, gae_lambda= 0.95, clip_range= 0.2, ent_coef= 0.0, vf_coef= 0.5, max_grad_norm= 0.5, use_sde= False, sde_sample_freq= -1, target_kl= 0.01, seed= None, device= 'auto', _init_setup_model= True, policy_kwargs= None, full_tensorboard_log= False)\n",
    "model1.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save = ('mario_model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the A2C model\n",
    "model2 = A2C('CnnPolicy', env, verbose=1, tensorboard_log=logs_dir, learning_rate= 0.03 n_steps= 256, batch_size= 256, n_epochs= 10, gamma= 0.99, gae_lambda= 0.95, clip_range= 0.2, ent_coef= 0.0, vf_coef= 0.5, max_grad_norm= 0.5, use_sde= False, sde_sample_freq= -1, target_kl= 0.01, seed= None, device= 'auto', _init_setup_model= True, policy_kwargs= None, full_tensorboard_log= False)\n",
    "model2.learn(total_timesteps=1000000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save = ('mario_model2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the performance of both models and compare\n",
    "results = load_results(logs_dir)\n",
    "plt.plot(results['timesteps'], results['r'], label='PPO')\n",
    "results = load_results(logs_dir)\n",
    "plt.plot(results['timesteps'], results['r'], label='A2C')\n",
    "plt.xlabel('Timesteps')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Training Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table to compare the performance of both models\n",
    "from tabulate import tabulate\n",
    "table = [[\"Model\", \"Mean Reward\", \"Std Reward\"],\n",
    "         [\"PPO\", \"Mean Reward\", \"Std Reward\"],\n",
    "         [\"A2C\", \"Mean Reward\", \"Std Reward\"]]\n",
    "print(tabulate(table))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
